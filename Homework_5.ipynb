{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6X-yZb6YvfhH",
        "XvDQjUmNSuLZ",
        "3IKYRqKtxR-N"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyapande1/HW5/blob/main/Homework_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq0XoVUIVNLC"
      },
      "source": [
        "# Homework 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aqF485OxR-U"
      },
      "source": [
        "**Before you start:** Read Chapter on Naive Bayes and KNN in the textbook.\n",
        "\n",
        "**Note:** Please enter the code along with your comments in the **TODO** section.\n",
        "\n",
        "Alternative solutions are always welcomed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rion40Hwnf-B"
      },
      "source": [
        "# Part 1: K-Nearst-Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X-yZb6YvfhH"
      },
      "source": [
        "### Problem 2 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hgf-yTV9q2V"
      },
      "source": [
        "The objective is to classify the breast cancer data using K-NN classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zY4oMVmBMHg"
      },
      "source": [
        "**TODO1**\n",
        "\n",
        "Load the breast cancer data and rename the columns to the below fields in the same order\n",
        "\n",
        "Id, C_thickness, Cell_Size, Cell_Shape, Adhesion, E_Cell_Size, Bare_Nuclei, B_Chromatin, N_Nucleoli, Mitoses, Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kVAjSQpZYiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSlUegOdcT2I"
      },
      "source": [
        "**TODO 2**\n",
        "\n",
        "Plot the heatmap for the correlation coefficients with the target variable (Class)  and interpret your findings.\n",
        "\n",
        "\n",
        "Drop redundant columns and view summary of the dataset.\n",
        "Convert all the variables to numeric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GyYUSKkfjXG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2saGofKTEpiJ"
      },
      "source": [
        "**TODO 3**\n",
        "\n",
        "\n",
        "\n",
        "Considering the fundamental idea of k-NN, would you recommend data rescaling before model building? Why? \n",
        "\n",
        "If so, partition the data into 75% training and 25% validation set.\n",
        "\n",
        "Impute the missing values with the mean values of training data.\n",
        "Check if all the nulls are removed in both train and test dataset.\n",
        "\n",
        "Standardize the data.\n",
        "\n",
        "**Note:**   When you standardize the validation set, you need to use the training set's mean and variance. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mizcZxd1fips"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrGgm_3mJbIR"
      },
      "source": [
        "**TODO 4**\n",
        "\n",
        "Choose the best k from 1-10 based on the classification accuracy of different k values on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgT0rqC4fOAp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7nvr1AbXeMg"
      },
      "source": [
        "**TODO 5**\n",
        "\n",
        "For the chosen k, display the confusion matrix and evaluate the performance of the model using recall and precision.\n",
        "\n",
        "Check for overfitting and underfitting for the k chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UANl3XmTfPjZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvUBy4oPMMIf"
      },
      "source": [
        "**TODO 6**\n",
        "\n",
        "Classify the new record given below using the chosen k. \n",
        "\n",
        "1002945, 5, 4, 4, 5, 7, 10, 3, 2, 1\n",
        "\n",
        "Considering the size of the dataset, would you recommend data partition before scoring the new record? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtThVQwSfhow"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvDQjUmNSuLZ"
      },
      "source": [
        "### Problem 3 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96_tZKmHSuLZ"
      },
      "source": [
        "The data concerns city-cycle fuel consumption in miles per gallon (mpg). The objective is to use k-NN regression to predict the mpg with the given attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viZfphuao7EV"
      },
      "outputs": [],
      "source": [
        "# import the dataset \"auto_mpg.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2OO76SGo7EV"
      },
      "source": [
        "**TODO 1**\n",
        "\n",
        "Check the unique value of the variable \"car name\". \n",
        "\n",
        "Would you recommend keeping \"car name\" for prediction? Why? \n",
        "\n",
        "If not, eliminate the variable \"car name\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05R9WLrTpijX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8XjK4Fro7EW"
      },
      "source": [
        "**TODO 2**\n",
        "\n",
        "Convert the variable \"origin\" to dummy variables before modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnkWQelTpkZI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNx7iZ2aC8h1"
      },
      "source": [
        "**TODO 3**\n",
        "\n",
        "Partition the data into 75% training and 25% validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGAWagCrDGDA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcjzpN38o7EY"
      },
      "source": [
        "**TODO 4**\n",
        "\n",
        "Rescale the numeric data. Note that dummy variables should not be rescaled.\n",
        "\n",
        "**Note:** When you standardize the validation set, you need to use the training set's mean and variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcCDLUdwppqS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezvy0kvYo7EZ"
      },
      "source": [
        "**TODO 5**\n",
        "\n",
        "Choose the best k from 1-10 based on the MSE of different k values on the validation set. Explain the reason for your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agdd1CYPp2rC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLdN-jMIp4-2"
      },
      "source": [
        "**TODO 6**\n",
        "\n",
        "\n",
        "Score the validation set with the best k. Comment on the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmCunSm7p85I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "T5fTwsd1UKSN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IKYRqKtxR-N"
      },
      "source": [
        "### **Problem 4**##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8fENJRPk7P7"
      },
      "source": [
        "In this problem, we need to build a Naive Bayes model to classify whether a movie review is positive or negative. \n",
        "\n",
        "The given data is a subset of [the IMDB movie review dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n",
        "\n",
        "This might be your first time working with text mining. Therefore, the basic pre-processing steps are given below. \n",
        "\n",
        "**You have two major tasks:**\n",
        "\n",
        "* Go through the code and get to know the purpose of each preprocessing step. Summarize what a preprocessing step does when required.\n",
        "* Build a multinomial Naive Bayes model to classify the reviews."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Please remove # and run the following code if you have an error while importing the dataset\n",
        "# !pip install --upgrade openpyxl"
      ],
      "metadata": {
        "id": "gsuUyuEhrcQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8gmUJ3n8Mir"
      },
      "source": [
        "# Import the dataset\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "file = files.upload()\n",
        "df = pd.read_csv(\"IMDB Dataset_subset.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggt8c71U8MrD",
        "outputId": "7f5ae431-5205-4812-9d73-e0060bd6c653"
      },
      "source": [
        "# Packages required for preprocessing #\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer #for lemmatization\n",
        "import re #regular expression package\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMI-UZAQ9F-L"
      },
      "source": [
        "X = [row for row in df['review']] #list of reviews\n",
        "classes = df['sentiment'] #list of true classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4P3iseB_gIR"
      },
      "source": [
        "# Pre-process the data\n",
        "reviews = []\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "for review in range(0, len(X)):\n",
        "    # part 1\n",
        "    review = re.sub(r'[\\W_]', ' ', str(X[review])) \n",
        "    review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', review) \n",
        "    review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', review) \n",
        "    review = re.sub(r'\\s+', ' ', review, flags=re.I) \n",
        "    review = re.sub(r'^b\\s+', '', review) # if a review record is in bytes, the corresponding line will have a letter 'b' appended at the start)\n",
        "    review = review.lower()\n",
        "    review = re.sub(r'[0-9]+', '', review) \n",
        "\n",
        "    # part 2\n",
        "    review = review.split()\n",
        "    review = [lemmatizer.lemmatize(word) for word in review]\n",
        "    review = ' '.join(review)\n",
        "\n",
        "    reviews.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orhex-OXhkgV"
      },
      "source": [
        "\n",
        "**TODO 1**\n",
        "\n",
        "Explain the function that part 1 and part 2 achieve in the loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGnaXwAfBv8i"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXexNUV9CnXZ"
      },
      "source": [
        "# Continue with pre-processing\n",
        "vectorizer = CountVectorizer(stop_words = \"english\", max_df=0.7, min_df=5) \n",
        "texts = vectorizer.fit_transform(reviews).toarray()  \n",
        "vocab = vectorizer.vocabulary_ \n",
        "vocab = sorted(vocab.items(), key = lambda x: x[1])\n",
        "vocab = [v[0] for v in vocab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZPj-AWCC-YV"
      },
      "source": [
        "\n",
        "**TODO 2**\n",
        "\n",
        "What do \"texts\" and \"vocab\" represent? What is the relationship between them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOho20diDt6L"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkjhBNIlGFlu"
      },
      "source": [
        "**TODO 3**\n",
        "\n",
        "Partition the data into 80% training and 20% validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WeDE--MGMq5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpGQxB2GGUzz"
      },
      "source": [
        "**TODO 4**\n",
        "\n",
        "Build a multinomial Naive Bayes model on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJGgsv-JGvga"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyypSMm_GzGb"
      },
      "source": [
        "**Hint:** [Multinomial Naive Bayes with sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfIyk3vMGww0"
      },
      "source": [
        "**TODO 5**\n",
        "\n",
        "Evaluate the model performance with the training and validation set. Comment on the model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5TyEr9pHJGt"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQy7IX2xHK1-"
      },
      "source": [
        "**Hint:** [Classification report with sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIRgUPM8HiOK"
      },
      "source": [
        "**If you are interested (this part is not graded):**\n",
        "\n",
        "Explore one or two records that were misclassified. Check the original text, vectorized text, and comment on the possible reason why the record got misclassified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REELGHe8IFcZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}